{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9676aff-0122-4a81-bf31-1cb51d0418eb",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Whenever you explain how the LLM works, you usually start that it can't actually operate on *text*. It operates on the sequence of vectors but before that on the sequence of integer numbers (encoding the token ID). We can then trivially get a vector by just passing the integers through `nn.Embedding`. So, the text needs to be transformed into the sequence of numbers. For this, we need a tokenizer.\n",
    "\n",
    "## BPE\n",
    "\n",
    "Let's skip the old word or rule-based methods and jump straight into Byte-Pair Encoding (BPE). We'll skip for now how it's trained and focus on the inference.\n",
    "\n",
    "BPE has a vocabulary that matches a sequence of **bytes** to a token ID. So, when you have a Python string, it (more or less) just does `s.encode(\"utf-8\")` and then tries to greedily match the bytes to the longest possible tokens.\n",
    "\n",
    "Let's implement it. First, we need to have a way to store these tokens. The problem is that if we want to store a token `\\t`, if you just call `print(token)`, the result will be invisible. Even worse, some of the bytes are weird unprintable control characters if you just assume they are unicode symbols. So, the first step, wee need to match bytes (practically numbers from 0 to 255) to some nice printable characters.\n",
    "\n",
    "Most of the implementations use GPT-2 convension for it:\n",
    "1. All the bytes that correspond to ASCII punctuation and alphanumeric characters are mapped to themselves (the bytes from `!` to `~`)\n",
    "2. The Latin-1 supplement from `¡` to `¬` is also matched to itself.\n",
    "3. Also, another block of Latin-1 from `®` to `ÿ` is matched to itself. This ensures that all roughly normal \"visible symbols\" are matched to themselves. Nice. The tricky part is the next.\n",
    "4. Whenever you have a byte outside these characters, it is matched to the character corresponding to 256 + the number of these \"weird\" characters we've seen already when iterating from 0 to 255. This also ensures that whitespace, `\\t`, `\\n`, etc are mapped to some \"safe range of printable stuff\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01d5705-3b3a-4afb-993e-cb3bb60834bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_unicode() -> dict[int, str]:\n",
    "    \"\"\"\n",
    "    Creates an mapping from integers to `str` implementing the logic we described above\n",
    "    \"\"\"\n",
    "\n",
    "    # your code goes here\n",
    "\n",
    "\n",
    "def unicode_to_bytes() -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Inverse transformation\n",
    "    \"\"\"\n",
    "    # your code goes here\n",
    "\n",
    "\n",
    "B2U = bytes_to_unicode()\n",
    "U2B = unicode_to_bytes()\n",
    "\n",
    "\n",
    "def encode_bytes_to_visible(s: bytes) -> str:\n",
    "    return \"\".join(B2U[b] for b in s)\n",
    "\n",
    "\n",
    "def decode_visible_to_bytes(s: str) -> bytes:\n",
    "    return bytes(U2B[ch] for ch in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35bd668-e81a-4345-9664-a65ff3a8907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(B2U, dict)\n",
    "assert len(B2U) == 256\n",
    "assert B2U[0] == \"Ā\"\n",
    "assert B2U[20] == \"Ĕ\"\n",
    "assert B2U[65] == \"A\"\n",
    "assert B2U[255] == \"ÿ\"\n",
    "assert encode_bytes_to_visible(\" token\".encode(\"utf-8\")) == \"Ġtoken\"\n",
    "assert encode_bytes_to_visible(\"\\ntoken\".encode(\"utf-8\")) == \"Ċtoken\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3775fc-7745-483e-8f2f-7f05d0986cf0",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "The course was written in 2025 and at that time no other normalization except [NFC](https://en.wikipedia.org/wiki/Unicode_equivalence#Normalization) exists. Gotta be honest, a very Unicode-specific stuff that didn't dive too deep in but luckly it's literally one Python line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b99860-ef30-4916-a929-4b287bb4bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "\n",
    "def nfc_normalize(text: str) -> str:\n",
    "    return unicodedata.normalize(\"NFC\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4248508e-a690-43d2-a302-5810432ddc3e",
   "metadata": {},
   "source": [
    "## Pre-Tokenization\n",
    "\n",
    "Pre-tokenization is relatively straighforward but mostly overlooked process. This is mainly the source of difference between different models' BPE-based tokenizers. Pre-tokenization is doing the following: it takes the **text** (Python `str`) and splits it into *pre-tokens* (a list of `str`).\n",
    "\n",
    "The actual BPE is then applied only inside these pre-tokens and the result is then concatenated. This is done both when running the tokenizer and when training it. We haven't discussed the training process yet but it actually defines what merges we allow. If the pre-tokenizer always splits by the whitespace, the learned tokens will never contain more than one word (in English, German, Dutch, Serbian, Greek, Russian and many (all?) other Indo-European languages meaning. For, let's say, Chineese the result might be very different! In some sense, for Chineese, the standard GPT-2 BPE is kinda similar to sentencepiece) because two words will always end up in different pre-tokens, so they will be tokenized independently.\n",
    "\n",
    "The basic pre-tokenizer might just split by whitespace (but the whitespace must be preserved in the resulting element, so `str.split(\" \")` isn't a valid pre-tokenizer). It might be just `lambda s: [s]`, so we allow all the possible merges. But most often than not the pre-tokenizer is defined by the regexp. Here are some of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f7ed8c-f220-4134-8b6f-27c720313aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_PRETOKENIZE_REGEXP = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4_PRETOKENIZE_REGEXP = (\n",
    "    r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    ")\n",
    "QWEN_PRETOKENIZE_REGEXP = r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1f70f8-cc33-4221-815b-3bb7110f372a",
   "metadata": {},
   "source": [
    "I really suggest to play around with them [here](https://regex101.com/r/NRYk93/1).\n",
    "\n",
    "The problem with pre-tokenizers is literally because these regexps (regexp problem in general) are unreadable. Below is a decent explanation from GPT-5 Thinking of what is going on in Qwen regexp:\n",
    "\n",
    "\n",
    "### What a “pre-tokenization regex” is\n",
    "\n",
    "Before BPE/Unigram does subword merges, most modern tokenizers first **split the raw string into chunks** (words, numbers, punctuation, spaces, newlines, contractions, etc.). That split is driven by a **single big regex**. The subword algorithm then runs **inside each chunk** to produce final token IDs. So this regex determines what the model even has a chance to merge—change it and you change what statistics the BPE learns.\n",
    "\n",
    "Your pattern:\n",
    "\n",
    "```\n",
    "PRETOKENIZE_REGEX =\n",
    "(?i:'s|'t|'re|'ve|'m|'ll|'d)\n",
    "|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+\n",
    "|\\p{N}\n",
    "| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*\n",
    "|\\s*[\\r\\n]+\n",
    "|\\s+(?!\\S)\n",
    "|\\s+\n",
    "```\n",
    "\n",
    "This is an **ordered OR** (alternation). Engines match **left to right**, first thing that fits wins. `\\p{L}` = any Unicode letter, `\\p{N}` = any Unicode number. Comments below reference **branches** in order.\n",
    "\n",
    "#### 1) `(?i:'s|'t|'re|'ve|'m|'ll|'d)`\n",
    "\n",
    "* **Meaning:** Contraction suffixes, case-insensitive, as atomic tokens.\n",
    "* **Why first:** You want `\"I'm\"` → `I` + `’m`, `\"CAN'T\"` → `CAN` + `’T`, not one big “word”.\n",
    "* **Examples:**\n",
    "\n",
    "  * `\"It’s\"` → `It` + `’s`\n",
    "  * `\"YOU’LL\"` → `YOU` + `’LL`\n",
    "\n",
    "#### 2) `[^\\r\\n\\p{L}\\p{N}]?\\p{L}+`\n",
    "\n",
    "* **Meaning:** A **word** (`\\p{L}+`) optionally **prefixed by one non-letter/number that’s not CR/LF** (often opening punctuation).\n",
    "* **Effect:** Binds an opening mark to the word: `“Hello` is one chunk, the closing `”` will be handled later.\n",
    "* **Examples:**\n",
    "\n",
    "  * `“Hello` → one chunk (leading `“` + `Hello`)\n",
    "  * `(world` → one chunk\n",
    "  * `foo` → one chunk\n",
    "\n",
    "#### 3) `\\p{N}`\n",
    "\n",
    "* **Meaning:** **One** numeric character.\n",
    "* **Design choice:** Single-digit chunks let BPE learn frequent multi-digit merges statistically (`2025` becomes `2|0|2|5` pre-BPE, then merges to `2025` if common). If this were `\\p{N}+`, the whole number would be a pretoken chunk and BPE would have less leverage.\n",
    "* **Examples:** `2025` → `2 | 0 | 2 | 5` (four chunks at the pre-token stage)\n",
    "\n",
    "#### 4) ` ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*`\n",
    "\n",
    "* **Meaning:** Optional leading space, then **one or more symbols/punct that are neither letters, numbers, nor whitespace**, then optional CR/LF.\n",
    "* **Effect:** Groups runs like `—–…`, `!!!`, emoji sequences, or `:)`, and **captures a following newline** if present (useful for end-of-line punctuation).\n",
    "* **Examples:**\n",
    "\n",
    "  * `\" —\"` → one chunk (leading space + em dash)\n",
    "  * `\"!!!\\n\"` → one chunk (bang-run + newline)\n",
    "  * `\"🙂👍\"` → one chunk\n",
    "\n",
    "#### 5) `\\s*[\\r\\n]+`\n",
    "\n",
    "* **Meaning:** Blank-line / line-break chunk: optional spaces then one or more CR/LF.\n",
    "* **Why separate from 4):** This catches **pure** line breaks (including spaces before them) when there isn’t a symbol run just before.\n",
    "* **Examples:**\n",
    "\n",
    "  * `\"\\n\"` or `\"   \\r\\n\"` → one chunk\n",
    "\n",
    "#### 6) `\\s+(?!\\S)`\n",
    "\n",
    "* **Meaning:** Whitespace **not followed by a non-space** → trailing spaces at end of line/string.\n",
    "* **Why:** Trailing spaces get isolated so they don’t glom onto the next word on the next line.\n",
    "* **Example:** `\"end   \\n\"` → the `   ` before `\\n` is one chunk here.\n",
    "\n",
    "#### 7) `\\s+`\n",
    "\n",
    "* **Meaning:** Any other whitespace (the general “leftovers” case).\n",
    "* **Examples:** Inter-word spaces when none of the earlier branches applied.\n",
    "\n",
    "---\n",
    "\n",
    "#### How a line is split (walkthrough)\n",
    "\n",
    "Input:\n",
    "`He said, “It’s 2025—finally!”\\n\\nOK.`\n",
    "\n",
    "Pre-token chunks (conceptually):\n",
    "\n",
    "1. `He` (branch 2)\n",
    "2. ` ` (branch 7)\n",
    "3. `said` (2)\n",
    "4. `,` (4)\n",
    "5. ` ` (7)\n",
    "6. `“It` (2; leading open quote bound to word)\n",
    "7. `’s` (1; contraction)\n",
    "8. ` ` (7)\n",
    "9. `2 | 0 | 2 | 5` (3,3,3,3)\n",
    "10. `—finally` (2; em dash bound to word because of the optional leading non-letter)\n",
    "11. `!”` (4; symbol run)\n",
    "12. `\\n` (5)\n",
    "13. `\\n` (5)\n",
    "14. `OK` (2)\n",
    "15. `.` (4)\n",
    "\n",
    "BPE then merges inside these pieces (e.g., `'s`, common words, frequent digit sequences, frequent emoji runs, etc.).\n",
    "\n",
    "> Note on engines: `\\p{…}` classes and that scoped `(?i: … )` are supported in PCRE/Oniguruma/Rust-regex/`regex` (PyPI). Python’s built-in `re` **does not** support `\\p{L}/\\p{N}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aba04ef-73c5-45e7-b25c-dbe3d5bbeeaa",
   "metadata": {},
   "source": [
    "Ok, enough yapping, let's implement this one-liner function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d6053f-5c80-48c2-8c5f-1df24c09ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re  # note that it's not the standard re. You need to actually add it to dependencies\n",
    "\n",
    "\n",
    "def pretokenize(text: str, pattern: str = QWEN_PRETOKENIZE_REGEXP) -> list[str]:\n",
    "    # your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cfa7fa-6c70-4d72-8747-c8e703800127",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pretokenize(\n",
    "    \"...I know he dyin' (oh my, oh my God) 6-7, I just bipped right on the highway (Bip, bip)\", QWEN_PRETOKENIZE_REGEXP\n",
    ") == [\n",
    "    \"...\",\n",
    "    \"I\",\n",
    "    \" know\",\n",
    "    \" he\",\n",
    "    \" dyin\",\n",
    "    \"'\",\n",
    "    \" (\",\n",
    "    \"oh\",\n",
    "    \" my\",\n",
    "    \",\",\n",
    "    \" oh\",\n",
    "    \" my\",\n",
    "    \" God\",\n",
    "    \")\",\n",
    "    \" \",\n",
    "    \"6\",\n",
    "    \"-\",\n",
    "    \"7\",\n",
    "    \",\",\n",
    "    \" I\",\n",
    "    \" just\",\n",
    "    \" bipped\",\n",
    "    \" right\",\n",
    "    \" on\",\n",
    "    \" the\",\n",
    "    \" highway\",\n",
    "    \" (\",\n",
    "    \"Bip\",\n",
    "    \",\",\n",
    "    \" bip\",\n",
    "    \")\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39288d84-fc86-492a-b794-f4188f1c54a3",
   "metadata": {},
   "source": [
    "## Actual BPE Tokenization\n",
    "\n",
    "### Training\n",
    "\n",
    "On a high level, BPE does the following: we start with a set of 256 tokens which are raw bytes. Then, we calculate the number of all pairs of tokens in the training corpus. After we've done it, we take the pair with the most occurences in the dataset, merge it into a new token, and calculate the pairs again. Repeat until we can't merge anything or we reached the required vocabulary size. The training process might be explained with more details but actually this is so intuitive already so I don't know what else to write. *Remember that tokenization is done inside the byte-encoded pre-tokens.*\n",
    "\n",
    "As a result, we have a vocabulary of tokens and the list of merges of tokens in chronological order when they happened. These are the `vocab.json` and `merges.txt` from HuggingFace.\n",
    "\n",
    "### Inference\n",
    "\n",
    "We take a list of pre-tokens. For each pre-token, we first split it into the set of \"tokens\" which are bytes, similarly to training. Then, we find all the pairs of tokens that exist in the merges, so they can be merged into a single token. There can be many of them, so which one to actually apply? If you can think about it, it will highly affect the result. Let's say we want to tokenize the word `and`. We start with `(\"a\", \"n\", \"d\")`. What if we have both `(\"a\", \"n\")` and `(\"n\", d\")` merges? The result would be different depending on the path we take. So, to make the tokenization stable, we take **the earliest merge** that happened during training. Remember, we support it during the training procedure. Then, we repeat the process until we can't merge anything anymore.\n",
    "\n",
    "The result for the whole text is just a concatenation of results for all the pre-tokens.\n",
    "\n",
    "Let's implement it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666ef6f7-923e-408f-8796-49b781aed7ec",
   "metadata": {},
   "source": [
    "We will do proper typings, so introducing these makes the code a bit more readable IMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d60851-82de-47bb-a886-d7a168d95157",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pair = tuple[str, str]\n",
    "Token = tuple[\n",
    "    str, ...\n",
    "]  # a pre-token that is represented as a set of \"symbols\" that at first are bytes but then will become actual tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4259910c-5997-4a6f-8242-7e2833cacd68",
   "metadata": {},
   "source": [
    "Some helper functions that will allow us to write a readable implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff695ed7-53a6-465e-bf13-55ee767105af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_pairs(token: Token) -> set[Pair]:\n",
    "    \"\"\"\n",
    "    Given a pre-token as a tuple of \"symbols\" (which at first are visible byte-chars), calculate the set of all the pairs of them.\n",
    "    \"\"\"\n",
    "    return {(token[i], token[i + 1]) for i in range(len(token) - 1)}\n",
    "\n",
    "\n",
    "def compute_pair_stats(corpus: list[Token]) -> Counter[Pair]:\n",
    "    \"\"\"\n",
    "    Reurns a counter with frequencies of all pairs of symbols in the corpus\n",
    "    \"\"\"\n",
    "    stats: Counter[Pair] = Counter()\n",
    "    for token in corpus:\n",
    "        stats.update(get_pairs(token))\n",
    "    return stats\n",
    "\n",
    "\n",
    "def merge_token(token: Token, pair: Pair, new_symbol: str) -> Token:\n",
    "    \"\"\"\n",
    "    Given a pre-token as tuple of symbols and a pair of two symbols that will become a new token,\n",
    "    replaces occurances when they are together on after another with a new symbol\n",
    "    \"\"\"\n",
    "    # your code goes here\n",
    "\n",
    "\n",
    "def replace_best_pair(corpus_tokens: list[Token], best_pair: Pair, symbol: str) -> list[Token]:\n",
    "    \"\"\"\n",
    "    Replaces a pair of tokens in all the pre-tokens in the training corpus with a new token.\n",
    "    God I hate this terminology.\n",
    "    \"\"\"\n",
    "    return [merge_token(token, best_pair, symbol) if len(token) >= 2 else token for token in corpus_tokens]\n",
    "\n",
    "\n",
    "assert merge_token((\"a\", \"b\", \"c\", \"b\"), (\"b\", \"c\"), \"bc\") == (\"a\", \"bc\", \"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb3739b-d2fe-4570-9071-c5dbc7d6dd4b",
   "metadata": {},
   "source": [
    "BPE inference! Functional way by passing the vocab and merges as function arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c1c668-fbe0-458a-aee5-c6f772012360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_pair(pairs: set[Pair], merge_ranks: dict[Pair, int]) -> Pair | None:\n",
    "    \"\"\"\n",
    "    Finds the pair with a minimum rank in merges, so we can replace a pair of tokens with one token\n",
    "    \"\"\"\n",
    "    # your code goes here\n",
    "\n",
    "\n",
    "def bpe_inference(pre_token: str, vocab: dict[str, int], merge_ranks: dict[Pair, int]) -> Token:\n",
    "    \"\"\"\n",
    "    Given a byte-encoded pre-token, run BPE inference on it to get a list of tokens out of it\n",
    "    \"\"\"\n",
    "    if not pre_token:\n",
    "        return []\n",
    "\n",
    "    word: Token = tuple(pre_token)\n",
    "    if len(word) == 1:\n",
    "        return [word[0]]\n",
    "\n",
    "    while True:\n",
    "        pairs = ...\n",
    "        best_pair = ...\n",
    "        if best_pair is None:\n",
    "            break\n",
    "\n",
    "        new_symbol = ...\n",
    "        word = ...\n",
    "\n",
    "    return word\n",
    "\n",
    "\n",
    "assert bpe_inference(\"abcd\", {\"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3, \"bc\": 4}, {(\"b\", \"c\"): 0}) == (\"a\", \"bc\", \"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e5b3a-f935-4c36-817a-9e62bf46fa77",
   "metadata": {},
   "source": [
    "BPE training and a a HuggingFace-like interface for the tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760a0884-dd37-4e7a-95d8-d492d7ca781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: dict[str, int] | None = None,\n",
    "        merge_ranks: dict[Pair, int] | None = None,\n",
    "        pretokenizer_regexp: str = QWEN_PRETOKENIZE_REGEXP,\n",
    "    ):\n",
    "        self.vocab = vocab\n",
    "        self.merge_ranks = merge_ranks\n",
    "        self.pretokenizer_regexp = pretokenizer_regexp\n",
    "\n",
    "    def train(self, texts: list[str], vocab_size: int, is_testing: bool = False) -> \"BPETokenizer\":\n",
    "        \"\"\"\n",
    "        Trains a BPE\n",
    "        \"\"\"\n",
    "        # 1) Prepare initial \"corpus\" at byte-level (visible Unicode chars) per pretokenized piece\n",
    "        corpus: list[Token] = []\n",
    "        for text in texts:\n",
    "            text_normalized = ...\n",
    "            # your code goes here\n",
    "\n",
    "        if is_testing:\n",
    "            assert corpus == [\n",
    "                \"A\",\n",
    "                \"Ġsentence\",\n",
    "                \"A\",\n",
    "                \"Ġ\",\n",
    "                \"6\",\n",
    "                \"9\",\n",
    "                \"Ġnumber\",\n",
    "                \"ðĿĶĺðĿĶ«ðĿĶ¦ðĿĶłðĿĶ¬ðĿĶ¡ðĿĶ¢\",\n",
    "                \"ĠOK\",\n",
    "                \".\",\n",
    "                \"Tabs\",\n",
    "                \"ĉand\",\n",
    "                \"Ġnewlines\",\n",
    "                \"Ċ\",\n",
    "                \"are\",\n",
    "                \"Ġalso\",\n",
    "                \"Ġtokens\",\n",
    "                \".\",\n",
    "            ]\n",
    "\n",
    "        # 2) Initialize vocab with 256 byte symbols + (optional) special tokens\n",
    "        self.vocab: dict[str, int] = {}\n",
    "        next_token_id = 0\n",
    "\n",
    "        # Add 256 visible-byte symbols (as one-char strings)\n",
    "\n",
    "        # your code goes here\n",
    "\n",
    "        if is_testing:\n",
    "            assert len(self.vocab) == 256\n",
    "            assert self.vocab[\"Ġ\"] == 32\n",
    "\n",
    "        # Merge tokens until we reach the vocabulary size\n",
    "        step = 0\n",
    "        self.merge_ranks: dict[Pair, int] = {}\n",
    "\n",
    "        progress = tqdm()\n",
    "\n",
    "        while next_token_id < vocab_size:\n",
    "            pair_stats = ...\n",
    "            if not pair_stats:\n",
    "                break  # cannot merge further (all tokens are singletons)\n",
    "\n",
    "            # Deterministic best: highest freq, tie-break lexicographically\n",
    "            max_freq = max(pair_stats.values())\n",
    "            candidate_pairs = [p for p, c in pair_stats.items() if c == max_freq]\n",
    "            best_pair = min(candidate_pairs)  # stable tie-break\n",
    "            new_symbol = \"\".join(best_pair)  # concatenation of the two symbols\n",
    "\n",
    "            if new_symbol not in self.vocab:\n",
    "                # your code goes here\n",
    "            else:\n",
    "                # I actually haven't given a thought on whether this can indeed happen or not. It seems it shouldn't\n",
    "                self.merge_ranks[best_pair] = len(self.merge_ranks)\n",
    "\n",
    "            # Replace the pair with a new token in the corpus\n",
    "            corpus = ...\n",
    "            step += 1\n",
    "            progress.update(1)\n",
    "            progress.set_description(f\"Merges: {step}, vocab={next_token_id}/{vocab_size}\")\n",
    "\n",
    "        progress.close()\n",
    "        if is_testing:\n",
    "            assert self.vocab[\"Ċ\"] == 10\n",
    "            assert self.vocab[\"Ġalso\"] == 315\n",
    "            assert self.vocab[\"Ġnumber\"] == 319\n",
    "\n",
    "        return self\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        \"\"\"\n",
    "        Encodes a normal text into token IDs\n",
    "        \"\"\"\n",
    "        # Normalize\n",
    "        text = nfc_normalize(text)\n",
    "\n",
    "        ids: list[int] = []\n",
    "\n",
    "        # Pre-tokenize\n",
    "        pre_tokens = ...\n",
    "        for pre_token in pre_tokens:\n",
    "            # Encode the pre-token to visible representation of bytes\n",
    "            encoded = ...\n",
    "            # Run BPE to merge tokens\n",
    "            symbols = ...\n",
    "            for symbol in symbols:\n",
    "                ids.append(self.vocab[symbol])\n",
    "        return ids\n",
    "\n",
    "    def decode(self, token_ids: list[int]) -> str:\n",
    "        \"\"\"\n",
    "        Given a list of token IDs, returns a proper text they correspond to\n",
    "        \"\"\"\n",
    "        inv_vocab = {i: s for s, i in self.vocab.items()}\n",
    "        visible_bytes: list[str] = []\n",
    "        for token_id in token_ids:\n",
    "            ...\n",
    "        byte_string = ...\n",
    "        return byte_string.decode(\"utf-8\", errors=\"strict\")\n",
    "\n",
    "    def save_pretrained(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Store the tokenizer in a HuggingFace-compatible BPE format. We loose pre-tokenizer this way unfortunately.\n",
    "        Storing a regexp is too advanced for the HF ecosystem.\n",
    "        \"\"\"\n",
    "        vocab_path = os.path.join(path, \"vocab.json\")\n",
    "        merges_path = os.path.join(path, \"merges.txt\")\n",
    "\n",
    "        with open(vocab_path, \"w\") as f:\n",
    "            json.dump(self.vocab, f)\n",
    "\n",
    "        with open(merges_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join([f\"{a}\\t{b}\" for a, b in self.merge_ranks.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba17e2e-5924-4880-aede-4b49f4a0c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"A sentence\",\n",
    "    \"A 69 number\",\n",
    "    \"𝔘𝔫𝔦𝔠𝔬𝔡𝔢 OK.\",\n",
    "    \"Tabs\\tand newlines\\nare also tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d4c958-2216-40d4-8f33-94de976adfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizer()\n",
    "tokenizer.train(texts, 320, is_testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44029cdb-a01e-4436-a3a3-225ed248f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.encode(\"A sentence with a number\") == [65, 316, 32, 119, 105, 116, 104, 32, 97, 319]\n",
    "assert tokenizer.decode([65, 316, 32, 119, 105, 116, 104, 32, 97, 319]) == \"A sentence with a number\"\n",
    "s = \"This must ALWAYS work\"\n",
    "assert tokenizer.decode(tokenizer.encode(s)) == s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80fc5f8-4907-4ce6-b5ca-bf0f6ced87f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
