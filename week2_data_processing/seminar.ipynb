{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79bdc720-4a24-4de5-a9ca-e2801d8e36f7",
   "metadata": {},
   "source": [
    "# Map-Reduce\n",
    "\n",
    "A lot of data processing operation become very neat and scalable when expressed as **MapRecuce** operations.\n",
    "\n",
    "[MapReduce](https://en.wikipedia.org/wiki/MapReduce) is a system (protocol? concept?) in which the operations over the set of data entries (**rows** in a **table**) are represented through a sequence of **map** and **reduce** operations. The fact is that almost all the transformation over data could be expressed as a sequence of map and reduce operations. The operations themselves can easily be parallelized, so you can grok insane amounts of data by running the ops with some distributed system, for example, [YTSaurus](https://ytsaurus.tech/).\n",
    "\n",
    "What it means:\n",
    "1. **Map** is a function $f$ such as, if the table is $T$, then for each $x \\in T$, it maps $x$ into a set of new rows: $f(x_i) = \\{y_1, \\ldots, y_{j_i}\\}$. This function is then applied to all rows in the table and the result is flattened.\n",
    "2. **Reduce** function $g$ takes a *key* $k$ and a set of all rows $X_k$ that correspond to it and returns a set of new rows: $g(k, X_k) = \\{y_1, \\ldots, y_{j_k}\\}$\n",
    "\n",
    "Let's implement a toy MapReduce system. All the operations will be configures with `pydantic` config objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da17bcd-dca2-4af4-8665-c85ca637f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from typing import Any, Callable, Iterable, Iterator, Literal, TypeVar\n",
    "\n",
    "\n",
    "class TransformConfigBase(pydantic.BaseModel): ...\n",
    "\n",
    "\n",
    "class MapTransformConfigBase(TransformConfigBase): ...\n",
    "\n",
    "\n",
    "class MapReduceTransformConfigBase(TransformConfigBase): ...\n",
    "\n",
    "\n",
    "Key = TypeVar(\"Key\")\n",
    "Row = dict[Any, Any]\n",
    "\n",
    "\n",
    "class TransformBase:\n",
    "    def run(self, iterator: Iterable[dict[Any, Any]]) -> list[dict[Any, Any]]:\n",
    "        pass\n",
    "\n",
    "\n",
    "class MapTransformBase(TransformBase):\n",
    "    def __init__(self, map_fn: Callable[[Row], Iterator[Row]], config: MapTransformConfigBase):\n",
    "        self._map_fn = map_fn\n",
    "        self.config = config\n",
    "\n",
    "    def run(self, iterator: Iterable[Row]) -> list[Row]:\n",
    "        return [row for input_row in iterator for row in self._map_fn(input_row)]\n",
    "\n",
    "\n",
    "class MapReduceTransformBase(TransformBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        map_fn: Callable[[Row], Iterator[tuple[Key, Row]]],\n",
    "        reduce_fn: Callable[[Key, Iterable[Row]], Iterator[Row]],\n",
    "        config: MapReduceTransformConfigBase,\n",
    "    ) -> None:\n",
    "        self._map_fn = map_fn\n",
    "        self._reduce_fn = reduce_fn\n",
    "        self.config = config\n",
    "\n",
    "    def run(self, iterator: Iterable[Row]) -> list[Row]:\n",
    "        # 1) Map: produce (key, row) pairs\n",
    "        pairs: list[tuple[K, Row]] = [row for input_row in iterator for row in self._map_fn(input_row)]\n",
    "        if not pairs:\n",
    "            return []\n",
    "\n",
    "        # 2) Sort by key so groupby works correctly\n",
    "        pairs.sort(key=itemgetter(0))\n",
    "\n",
    "        # 3) Group by key and feed each group's rows into reduce_fn\n",
    "        out: list[Row] = []\n",
    "        for key, group in groupby(pairs, key=itemgetter(0)):\n",
    "            values_iter = [row for _, row in group]\n",
    "            out.extend(self._reduce_fn(key, values_iter))\n",
    "\n",
    "        # 4) Return the flattened reduce results\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daec743d-f9b5-40ae-99ae-593b6f660658",
   "metadata": {},
   "source": [
    "Let's start with a simple map example. Our table contains some columns and one of them contains text. We want to transform the table by changing the case of text in a specified column to either upper or lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1908cb-a3de-4ddc-b532-80b0444e32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangeCaseTransformConfig(MapTransformConfigBase):\n",
    "    type: Literal[\"change_case\"] = \"change_case\"\n",
    "    column: str\n",
    "    to_lower: bool = False\n",
    "\n",
    "\n",
    "class ChangeCaseTransform(MapTransformBase):\n",
    "    \"\"\"\n",
    "    Takes a dictionary and changes the case of value with the key `column`\n",
    "    \"\"\"\n",
    "\n",
    "    class _MapFn:\n",
    "        def __init__(self, config: ChangeCaseTransformConfig):\n",
    "            self.config = config\n",
    "\n",
    "        def __call__(self, row: Row) -> Iterator[Row]:\n",
    "            # your code goes here\n",
    "            yield row\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: ChangeCaseTransformConfig,\n",
    "    ) -> None:\n",
    "        super().__init__(self._MapFn(config), config)\n",
    "\n",
    "\n",
    "test_config = ChangeCaseTransformConfig(\n",
    "    column=\"content\",\n",
    "    to_lower=True,\n",
    ")\n",
    "test_input = [\n",
    "    {\"content\": \"A String\"},\n",
    "    {\"content\": \"abcc\"},\n",
    "    {\"content\": \"BCD\"},\n",
    "]\n",
    "\n",
    "assert ChangeCaseTransform(test_config).run(test_input) == [\n",
    "    {\"content\": \"a string\"},\n",
    "    {\"content\": \"abcc\"},\n",
    "    {\"content\": \"bcd\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b656d54-eaba-411a-a872-5c4a20d7bc4c",
   "metadata": {},
   "source": [
    "A classic example of a map-reduce operation is counting the number of occurences of all letters (chars) in the texts in a specified table's column. To do so, we first run a mapper that for each row produces multiple rows with the letter as a **key** and a column that stores the number of occurences of this letter in this particular input row. The reducer than just sums up the counters for each letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5662c-599f-442e-b219-98fb6b2983bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "class CountCharsTransformConfig(MapReduceTransformConfigBase):\n",
    "    \"\"\"\n",
    "    If `lowercase` is `False`, don't change the case,\n",
    "    otherwise, make the string lowercase first\n",
    "    \"\"\"\n",
    "\n",
    "    lowercase: bool = False\n",
    "    column: str\n",
    "\n",
    "\n",
    "class CountCharsTransform(MapReduceTransformBase):\n",
    "    \"\"\"\n",
    "    Takes a table with strings, optionally, casts them to the lower case,\n",
    "    produces a table with columns `letter` and `cnt` with the count of total occurences\n",
    "    of each character in the input table strings.\n",
    "    \"\"\"\n",
    "\n",
    "    class _MapFn:\n",
    "        def __init__(self, config: CountCharsTransformConfig):\n",
    "            self.config = config\n",
    "\n",
    "        def __call__(self, row: Row) -> Iterator[Row]:\n",
    "            # your code goes here\n",
    "\n",
    "            letters_count = ...\n",
    "            for letter, cnt in letters_count.items():\n",
    "                yield letter, {\"cnt\": cnt}\n",
    "\n",
    "    class _ReduceFn:\n",
    "        def __init__(self, config: CountCharsTransformConfig):\n",
    "            self.config = config\n",
    "\n",
    "        def __call__(self, key: Key, rows: Iterable[Row]) -> Iterator[Row]:\n",
    "            # your code goes here\n",
    "            pass\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: CountCharsTransformConfig,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            self._MapFn(config),\n",
    "            self._ReduceFn(config),\n",
    "            config,\n",
    "        )\n",
    "\n",
    "\n",
    "test_config = CountCharsTransformConfig(\n",
    "    column=\"content\",\n",
    "    lowercase=True,\n",
    ")\n",
    "\n",
    "test_input = [\n",
    "    {\"content\": \"A String\"},\n",
    "    {\"content\": \"abcc\"},\n",
    "    {\"content\": \"BCD\"},\n",
    "]\n",
    "\n",
    "assert CountCharsTransform(test_config).run(test_input) == [\n",
    "    {\"letter\": \" \", \"cnt\": 1},\n",
    "    {\"letter\": \"a\", \"cnt\": 2},\n",
    "    {\"letter\": \"b\", \"cnt\": 2},\n",
    "    {\"letter\": \"c\", \"cnt\": 3},\n",
    "    {\"letter\": \"d\", \"cnt\": 1},\n",
    "    {\"letter\": \"g\", \"cnt\": 1},\n",
    "    {\"letter\": \"i\", \"cnt\": 1},\n",
    "    {\"letter\": \"n\", \"cnt\": 1},\n",
    "    {\"letter\": \"r\", \"cnt\": 1},\n",
    "    {\"letter\": \"s\", \"cnt\": 1},\n",
    "    {\"letter\": \"t\", \"cnt\": 1},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f53d4-1559-4baf-85e3-dba4aefc3d00",
   "metadata": {},
   "source": [
    "# Language Modeling Datasets\n",
    "\n",
    "Language models don't operate on text, they operate on tokens. We know that. However, only the list of tokens is usually not sufficient for the model training. I mean it kinda is but not **always**. Sometimes we want two additional things:\n",
    "1. **Loss mask.** It shows on which tokens we need to calculate the loss. Helps in two cases: 1) whenever we have padding somewhere in the tensors, we don't want to calculate the loss on pads; 2) when the data has some \"inputs\" and \"outputs\" and only the output is written by the LM. Then we want to calculate loss only on what LM is actually required to predict.\n",
    "2. **Position IDs**. Just a sequential numbers indicating the position of a current token. They're required in case we have truncated sequences and learned/cosine postional encodings. They are not strictly required with RoPE but it's still very helpful to have them stored for auxulary needs we'll cover later.\n",
    "\n",
    "### What do they actually look like?\n",
    "\n",
    "Position IDs are simple. Just an `arange` from 0 to the number of tokens. The loss mask, however, depends on what we want. Let's cover some standard situations.\n",
    "\n",
    "#### Pre-Training\n",
    "\n",
    "In pre-training, loss mask is all `True`s. There might be some issues with padding when we implement specific packing algorithms but on an individual-example level, it's all `True`s.\n",
    "\n",
    "#### Fill-In-the-Middle\n",
    "\n",
    "In FIM, the loss mask might be still all `True`s as in pre-training but we might have an option to only calculate the loss on **middle**, because this is what we actually expect the model to generate, everything else is the input.\n",
    "\n",
    "#### Supervised Fine-Tuning of Instruct Models\n",
    "\n",
    "In instruct-SFT, we usually only calculate the loss on the assistant's repsonses.\n",
    "\n",
    "\n",
    "\n",
    "Let's implement all these strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11975e5-f92b-48ce-87ed-cce510ffbeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from jaxtyping import UInt32, Bool\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer\n",
    "\n",
    "TokenizedRow = dict[str, UInt32[np.typing.ArrayLike, \"example_len\"] | Bool[np.typing.ArrayLike, \"example_len\"]]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"JetBrains/Mellum-4b-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f03291-18ed-4aa3-9785-54358bb909c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_processing(\n",
    "    row: Row,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    ") -> TokenizedRow:\n",
    "    \"\"\"\n",
    "    Takes a row with a column `content`, returns a new row with three columns:\n",
    "    1. `input_ids`\n",
    "    2. `position_ids`\n",
    "    3. `loss_mask`\n",
    "\n",
    "    All of them are NumPy arrays. The tokens IDs and positions fit into UInt32, so\n",
    "    use it as a dtype. For loss mask, use `bool`.\n",
    "    \"\"\"\n",
    "    text = row[\"content\"]\n",
    "    # your code goes here\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"position_ids\": position_ids,\n",
    "        \"loss_mask\": loss_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "result = pretrain_processing({\"content\": \"A sample text\"}, tokenizer)\n",
    "assert np.allclose(result[\"input_ids\"], np.array([59, 5875, 1378, 0], dtype=np.uint32))\n",
    "assert np.allclose(result[\"loss_mask\"], np.array([True, True, True, True], dtype=np.bool))\n",
    "assert np.allclose(result[\"position_ids\"], np.array([0, 1, 2, 3], dtype=np.uint32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7602e798-7e81-4090-959d-945f2bf17059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fim_processing(\n",
    "    row: Row,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    only_middle_not_masked: bool = False,\n",
    "    fim_format: Literal[\"spm\", \"psm\"] = \"spm\",\n",
    ") -> TokenizedRow:\n",
    "    \"\"\"\n",
    "    The input is a row with three columns: `prefix`, `suffix`, `middle`.\n",
    "    The output is the same as with pre-training. The FIM format indicates the type of FIM we're using.\n",
    "    SPM stands for `<fim_suffix>{suffix}<fim_prefix>{prefix}<fim_middle>{middle}`,\n",
    "    PSM for `<fim_prefix>{prefix}<fim_suffix>{suffix}<fim_middle>{middle}`.\n",
    "    \"\"\"\n",
    "    assert len(tokenizer.encode(\"<fim_prefix>\")) == 1, \"Tokenizer doesn't have special FIM tokens\"\n",
    "    assert len(tokenizer.encode(\"<fim_suffix>\")) == 1, \"Tokenizer doesn't have special FIM tokens\"\n",
    "    assert len(tokenizer.encode(\"<fim_middle>\")) == 1, \"Tokenizer doesn't have special FIM tokens\"\n",
    "    fim_prefix_id = tokenizer.encode(\"<fim_prefix>\")[0]\n",
    "    fim_suffix_id = tokenizer.encode(\"<fim_suffix>\")[0]\n",
    "    fim_middle_id = tokenizer.encode(\"<fim_middle>\")[0]\n",
    "    prefix = row[\"prefix\"]\n",
    "    suffix = row[\"suffix\"]\n",
    "    middle = row[\"middle\"]\n",
    "\n",
    "    # your code goes here\n",
    "\n",
    "\n",
    "example = {\n",
    "    \"prefix\": \"import numpy as\",\n",
    "    \"middle\": \"np\",\n",
    "    \"suffix\": \"\\nimport pandas as pd\",\n",
    "}\n",
    "result_psm = fim_processing(example, tokenizer, only_middle_not_masked=False, fim_format=\"psm\")\n",
    "assert np.allclose(\n",
    "    result_psm[\"input_ids\"], np.array([1, 653, 9440, 609, 3, 225, 653, 17984, 609, 8559, 2, 3057, 0], dtype=np.uint32)\n",
    ")\n",
    "assert np.allclose(result_psm[\"loss_mask\"], np.ones(13, dtype=np.bool))\n",
    "assert np.allclose(result_psm[\"position_ids\"], np.arange(13, dtype=np.uint32))\n",
    "\n",
    "result_spm = fim_processing(example, tokenizer, only_middle_not_masked=False, fim_format=\"spm\")\n",
    "assert np.allclose(\n",
    "    result_spm[\"input_ids\"], np.array([3, 225, 653, 17984, 609, 8559, 1, 653, 9440, 609, 2, 3057, 0], dtype=np.uint32)\n",
    ")\n",
    "assert np.allclose(result_spm[\"loss_mask\"], np.ones(13, dtype=np.bool))\n",
    "assert np.allclose(result_spm[\"position_ids\"], np.arange(13, dtype=np.uint32))\n",
    "\n",
    "result_spm_masked = fim_processing(example, tokenizer, only_middle_not_masked=True, fim_format=\"spm\")\n",
    "assert np.allclose(\n",
    "    result_spm_masked[\"input_ids\"],\n",
    "    np.array([3, 225, 653, 17984, 609, 8559, 1, 653, 9440, 609, 2, 3057, 0], dtype=np.uint32),\n",
    ")\n",
    "assert np.allclose(\n",
    "    result_spm_masked[\"loss_mask\"], np.concatenate([np.zeros(11, dtype=np.bool), np.ones(2, dtype=np.bool)])\n",
    ")\n",
    "assert np.allclose(result_spm_masked[\"position_ids\"], np.arange(13, dtype=np.uint32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87963e80-174f-462d-971b-df51f73fbbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sft_processing(\n",
    "    row: Row,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    ") -> TokenizedRow:\n",
    "    \"\"\"\n",
    "    The row has a column `content` which is a list of turns in the\n",
    "    format of `[{\"role\": user/assistant, \"content\": msg_text}]`.\n",
    "    \"\"\"\n",
    "    assert len(tokenizer.encode(\"<user>\")) == 1, \"Tokenizer chat format is not supported or doesn't exist\"\n",
    "    assert len(tokenizer.encode(\"</user>\")) == 1, \"Tokenizer chat format is not supported or doesn't exist\"\n",
    "    assert len(tokenizer.encode(\"<assistant>\")) == 1, \"Tokenizer chat format is not supported or doesn't exist\"\n",
    "    assert len(tokenizer.encode(\"</assistant>\")) == 1, \"Tokenizer chat format is not supported or doesn't exist\"\n",
    "    user_start_id = tokenizer.encode(\"<user>\")[0]\n",
    "    user_end_id = tokenizer.encode(\"</user>\")[0]\n",
    "    assistant_start_id = tokenizer.encode(\"<assistant>\")[0]\n",
    "    assistant_end_id = tokenizer.encode(\"</assistant>\")[0]\n",
    "    conversation = row[\"content\"]\n",
    "\n",
    "    # your code goes here\n",
    "\n",
    "\n",
    "example = {\"content\": [{\"role\": \"user\", \"content\": \"Hi!\"}, {\"role\": \"assistant\", \"content\": \"How can I help you?\"}]}\n",
    "result = sft_processing(example, tokenizer)\n",
    "assert np.allclose(\n",
    "    result[\"input_ids\"], np.array([21, 4479, 27, 22, 23, 7166, 867, 391, 2739, 691, 57, 24, 0], dtype=np.uint32)\n",
    ")\n",
    "assert np.allclose(result[\"loss_mask\"], np.concatenate([np.zeros(4, dtype=np.bool), np.ones(9, dtype=np.bool)]))\n",
    "assert np.allclose(result[\"position_ids\"], np.arange(13, dtype=np.uint32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b585ab4-3d23-407c-8990-ae0a891f4bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcessingStrategy = Literal[\"pretrain\", \"fim\", \"sft\"]\n",
    "\n",
    "\n",
    "def process_row(row: Row, tokenizer: PreTrainedTokenizer, strategy: ProcessingStrategy, **kwargs) -> TokenizedRow:\n",
    "    if strategy == \"pretrain\":\n",
    "        return pretrain_processing(row, tokenizer)\n",
    "    elif strategy == \"fim\":\n",
    "        return fim_processing(row, tokenizer, **kwargs)\n",
    "    elif strategy == \"sft\":\n",
    "        return sft_processing(row, tokenizer)\n",
    "    else:\n",
    "        raise ValueError(f\"Strategy `{strategy} is not supported`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a91cb04-ad0c-417f-9046-4eebdbaf4de5",
   "metadata": {},
   "source": [
    "# Examples Packing\n",
    "\n",
    "You may notice that all the text have different lengths. However, we need to somehow feed them all into the model. We can't just leave the examples as is because in this case we won't be able to build a batch (unless using batch size 1 and PyTorch eager) since all tensors will have different shapes. We could pad them all to some pre-defined number of tokens (context size) but this will be super inefficient: an example of the length 1 would have, let's say, 8191 pad tokens.\n",
    "\n",
    "To solve this issue, the examples are usually *packed* into tensors of the `seq_len` size. So, multiple short sequences are getting concatenated. Will it mean that when looking at the second example, the model would see the tokens from the first one? No! To avoid it, we'll build an attention mask matrix. It will be used in the attention layers, so, for each token, the attention will be calculated only on the previous tokens and only on those that correspond to the same example in the packing.\n",
    "\n",
    "There are two main strategies used for packing:\n",
    "1. **Dense (pre-training).** We greedily concatenate all the sequences and cut them into `seq_len` pieces.\n",
    "2. **No incomplete (fine-tuning).** We just try to concatenate the examples until we exceed the `seq_len`. When we can't fit a new sequence, *pad the previous ones*.\n",
    "\n",
    "Let's implement them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67acf324-d5e1-48e8-b0a6-748f288d28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_fix_helper(postion_ids: UInt32[np.typing.ArrayLike, \"example_len\"]) -> UInt32[np.ndarray, \"example_len\"]:\n",
    "    \"\"\"\n",
    "    Rebase each segment (between decreases) so it starts at 0.\n",
    "    Example: [2,3,4,0,1,2,3] -> [0,1,2,0,1,2,3]\n",
    "             [4,0,1,0]       -> [0,0,1,0]\n",
    "    \"\"\"\n",
    "    ids = np.asarray(postion_ids, dtype=np.uint32)\n",
    "    n = ids.shape[0]\n",
    "    if n == 0:\n",
    "        return ids\n",
    "\n",
    "    # Find starts of new segments where the sequence decreases.\n",
    "    disc = np.flatnonzero(ids[1:] < ids[:-1]) + 1\n",
    "    starts = np.concatenate(([0], disc))\n",
    "\n",
    "    out = ids.astype(np.int64, copy=True)  # avoid uint underflow\n",
    "    for i, s in enumerate(starts):\n",
    "        e = starts[i + 1] if i + 1 < len(starts) else n\n",
    "        base = int(out[s])\n",
    "        out[s:e] -= base\n",
    "\n",
    "    return out.astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c08040-9d97-40ac-b3d7-750c4f0ae7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_dense(rows: Iterable[TokenizedRow], seq_len: int) -> list[TokenizedRow]:\n",
    "    \"\"\"\n",
    "    Greedily packs all the rows into seq_len tensors, dropping the trailing examples if there are any\n",
    "    \"\"\"\n",
    "    input_ids = np.concatenate([row[\"input_ids\"] for row in rows])\n",
    "    loss_mask = np.concatenate([row[\"loss_mask\"] for row in rows])\n",
    "    position_ids = np.concatenate([row[\"position_ids\"] for row in rows])\n",
    "    # your code goes here\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input_ids\": np.array([1, 2, 3, 4, 5], dtype=np.uint32),\n",
    "        \"loss_mask\": np.ones(5, dtype=np.bool),\n",
    "        \"position_ids\": np.arange(5, dtype=np.uint32),\n",
    "    },\n",
    "    {\n",
    "        \"input_ids\": np.array([0, 1], dtype=np.uint32),\n",
    "        \"loss_mask\": np.ones(2, dtype=np.bool),\n",
    "        \"position_ids\": np.arange(2, dtype=np.uint32),\n",
    "    },\n",
    "    {\n",
    "        \"input_ids\": np.array([10, 10], dtype=np.uint32),\n",
    "        \"loss_mask\": np.ones(2, dtype=np.bool),\n",
    "        \"position_ids\": np.arange(2, dtype=np.uint32),\n",
    "    },\n",
    "    {\n",
    "        \"input_ids\": np.array([11], dtype=np.uint32),\n",
    "        \"loss_mask\": np.ones(1, dtype=np.bool),\n",
    "        \"position_ids\": np.arange(1, dtype=np.uint32),\n",
    "    },\n",
    "]\n",
    "\n",
    "result = pack_dense(examples, 4)\n",
    "assert np.allclose(result[0][\"input_ids\"], np.array([1, 2, 3, 4], dtype=np.uint32))\n",
    "assert np.allclose(result[1][\"input_ids\"], np.array([5, 0, 1, 10], dtype=np.uint32))\n",
    "assert np.allclose(result[0][\"position_ids\"], np.array([0, 1, 2, 3], dtype=np.uint32))\n",
    "assert np.allclose(result[1][\"position_ids\"], np.array([0, 0, 1, 0], dtype=np.uint32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca416e3e-3aa9-45a0-ba53-e8ec474b0f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_no_incomplete(rows: Iterable[TokenizedRow], seq_len: int, pad_token_id: int) -> list[TokenizedRow]:\n",
    "    \"\"\"\n",
    "    Ensures there are no incomplete (truncated) examples. If an example exceeds seq_len, drop it. Pad the tensors\n",
    "    with pad_token_id, zeros in position_ids, and False in loss_mask. We don't want pads to be included in the gradient\n",
    "    calculation graphs.\n",
    "    \"\"\"\n",
    "    # your code goes here\n",
    "\n",
    "\n",
    "result = pack_no_incomplete(examples, 4, 0)\n",
    "assert np.allclose(result[0][\"input_ids\"], np.array([0, 1, 10, 10], dtype=np.uint32))\n",
    "assert np.allclose(result[0][\"position_ids\"], np.array([0, 1, 0, 1], dtype=np.uint32))\n",
    "assert np.allclose(result[0][\"loss_mask\"], np.array([True, True, True, True], dtype=np.bool))\n",
    "assert np.allclose(result[1][\"input_ids\"], np.array([11, 0, 0, 0], dtype=np.uint32))\n",
    "assert np.allclose(result[1][\"position_ids\"], np.array([0, 0, 0, 0], dtype=np.uint32))\n",
    "assert np.allclose(result[1][\"loss_mask\"], np.array([True, False, False, False], dtype=np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a9037-0293-40bd-8947-18a23231e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PackingStrategy = Literal[\"dense\", \"no_incomplete\"]\n",
    "\n",
    "\n",
    "def pack_sequences(\n",
    "    rows: Iterable[TokenizedRow], seq_len: int, pad_token_id: int, strategy: Literal[\"dense\", \"no_incomplete\"]\n",
    ") -> list[TokenizedRow]:\n",
    "    if strategy == \"dense\":\n",
    "        return pack_dense(rows, seq_len)\n",
    "    elif strategy == \"no_incomplete\":\n",
    "        return pack_no_incomplete(rows, seq_len, pad_token_id)\n",
    "    else:\n",
    "        raise ValueError(f\"Strategy `{strategy}` is not supported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df891e2c-57c3-4eb4-b32e-af97736904d0",
   "metadata": {},
   "source": [
    "# Using Map-Reduce for Preparing Datasets\n",
    "\n",
    "The tokenization + packing can be scaled by using MapReduce. In mapper, we tokenize the examples and assign them some \"group\" ID (e.g., a hash % n_groups). Then, we run the reduce by that group ID and pack the sequences that appeared in the same group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd6d84-55fc-42f2-8c52-646e80a6b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareForTrainingTransformConfig(MapReduceTransformConfigBase):\n",
    "    processing_strategy: ProcessingStrategy\n",
    "    packing_strategy: PackingStrategy\n",
    "    tokenizer_name: str\n",
    "    seq_len: int\n",
    "    pad_token_id: int | None = None\n",
    "    fim_kwargs: dict[str, Any] = {}\n",
    "    num_reduce_chunks: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aed28f-7dc6-4c1e-b9e7-6ae9e46d874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def array_hash(arr: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Calculate a hash of a NumPy array using its string representation.\n",
    "    Includes dtype and shape to ensure distinct arrays have distinct hashes.\n",
    "    \"\"\"\n",
    "    arr_bytes = arr.tobytes()\n",
    "    meta = f\"{arr.shape}-{arr.dtype}\".encode()\n",
    "    digest = hashlib.sha256(meta + arr_bytes).digest()\n",
    "    return int.from_bytes(digest, byteorder=\"big\")\n",
    "\n",
    "\n",
    "class PrepareForTrainingTransform(MapReduceTransformBase):\n",
    "    class _MapFn:\n",
    "        def __init__(self, config: PrepareForTrainingTransformConfig):\n",
    "            self.config = config\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "\n",
    "        def __call__(self, row: Row) -> Iterator[TokenizedRow]:\n",
    "            result = ...\n",
    "            # the key is the array hash mod number of reduce chunks\n",
    "            key = array_hash(result[\"input_ids\"]) % self.config.num_reduce_chunks\n",
    "            yield key, result\n",
    "\n",
    "    class _ReduceFn:\n",
    "        def __init__(self, config: PrepareForTrainingTransformConfig):\n",
    "            self.config = config\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "            self.pad_token_id = (\n",
    "                self.config.pad_token_id if self.config.pad_token_id is not None else tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        def __call__(self, key: Key, rows: Iterable[TokenizedRow]) -> Iterator[TokenizedRow]:\n",
    "            \"\"\"\n",
    "            Packs the sequences that ended up in the same reduce chunk\n",
    "            \"\"\"\n",
    "            yield from ...\n",
    "\n",
    "    def __init__(self, config: PrepareForTrainingTransformConfig):\n",
    "        super().__init__(\n",
    "            map_fn=self._MapFn(config),\n",
    "            reduce_fn=self._ReduceFn(config),\n",
    "            config=config,\n",
    "        )\n",
    "\n",
    "\n",
    "test_config = PrepareForTrainingTransformConfig(\n",
    "    processing_strategy=\"pretrain\",\n",
    "    packing_strategy=\"dense\",\n",
    "    tokenizer_name=\"JetBrains/Mellum-4b-base\",\n",
    "    seq_len=8,\n",
    "    num_reduce_chunks=2,\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\"content\": \"Hello, World!\"},\n",
    "    {\"content\": \"A test string\"},\n",
    "    {\"content\": \"import numpy as np\"},\n",
    "    {\"content\": \"import pandas as pd\"},\n",
    "]\n",
    "\n",
    "transform = PrepareForTrainingTransform(test_config)\n",
    "result = transform.run(examples)\n",
    "\n",
    "assert np.allclose(result[0][\"input_ids\"], np.array([10626, 38, 8098, 27, 0, 653, 17984, 609], dtype=np.uint32))\n",
    "assert np.allclose(result[1][\"input_ids\"], np.array([59, 247, 110, 95, 109, 110, 987, 0], dtype=np.uint32))\n",
    "assert np.allclose(result[0][\"position_ids\"], np.array([0, 1, 2, 3, 4, 0, 1, 2], dtype=np.uint32))\n",
    "assert np.allclose(result[1][\"position_ids\"], np.array([0, 1, 2, 3, 4, 5, 6, 7], dtype=np.uint32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f2f2b5-4de4-426c-ac57-74de5dc710e4",
   "metadata": {},
   "source": [
    "Let's try running it over a real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8a5ee8-3e4f-489b-b443-e702f5ef1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=\"sample-10BT\", split=\"train\", streaming=True)\n",
    "\n",
    "rows = []\n",
    "idx = 0\n",
    "for row in ds:\n",
    "    rows.append({\"content\": row[\"text\"]})\n",
    "    idx += 1\n",
    "    if idx > 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88521a79-5561-480b-b7fb-d96d361c9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PrepareForTrainingTransformConfig(\n",
    "    processing_strategy=\"pretrain\",\n",
    "    packing_strategy=\"dense\",\n",
    "    tokenizer_name=\"JetBrains/Mellum-4b-base\",\n",
    "    seq_len=1025,\n",
    "    num_reduce_chunks=4,\n",
    ")\n",
    "\n",
    "transform = PrepareForTrainingTransform(config)\n",
    "result = transform.run(rows)\n",
    "assert len(result) == 1053"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b3e11d-8df8-42ba-807d-5be3d96fde29",
   "metadata": {},
   "source": [
    "# Making a Dataset Object\n",
    "\n",
    "Now let's build an actual PyTorch dataset that we can use for training. For it, we'll need:\n",
    "1. Input tokens: the ones for which we'll predict the next ones (LM objective). All the tokens except the last because for the last we don't have the next.\n",
    "2. Target tokens (classification \"labels\"): the ground truth next tokens. All the tokens except the first. So, the input and label tokens are shifted by 1.\n",
    "3. Loss mask for labels\n",
    "4. Position IDs of input tokens\n",
    "5. Attention mask. A square matrix in which the entry $i$-th row contains information about whether the token $i$ should look at the token $j$ during attention calculation. It can easily be constructed from position IDs because they are just consecutive but when the $i+1$-th element isn't greater that $i$-th, this means we have the next sequence in the packing. Since before we pathched position IDs, so they always start from 0, we can just find all the zeros in position IDs and they will correspond to begginings of segments in the packing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd0fcf8-fe88-4f1b-8c01-a489371d7f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_attention_mask_from_positions(\n",
    "    position_ids: UInt32[np.typing.ArrayLike, \"seq_len\"],\n",
    ") -> Bool[np.typing.ArrayLike, \"seq_len seq_len\"]:\n",
    "    \"\"\"\n",
    "    Creates an attention mask matrix from `position_ids`\n",
    "\n",
    "    :param np.ndarray position_ids: An array representing the position IDs of tokens in a sequence.\n",
    "        It should have a shape of (seq_len,).\n",
    "    :return: An array representing the attention mask. It has a shape of (1, seq_len, seq_len).\n",
    "        The elements of the attention mask are binary values indicating whether each token in\n",
    "        the sequence should be attended to (True) or not (False).\n",
    "    \"\"\"\n",
    "    # your code goes here\n",
    "\n",
    "\n",
    "assert np.allclose(\n",
    "    make_attention_mask_from_positions(np.array([0, 1, 2, 0, 1], dtype=np.uint32)),\n",
    "    np.array(\n",
    "        [\n",
    "            [True, False, False, False, False],\n",
    "            [True, True, False, False, False],\n",
    "            [True, True, True, False, False],\n",
    "            [False, False, False, True, False],\n",
    "            [False, False, False, True, True],\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b300a-120a-4556-875d-7beb4f4889ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "Inputs = UInt32[np.typing.ArrayLike, \"seq_len\"]\n",
    "Labels = UInt32[np.typing.ArrayLike, \"seq_len\"]\n",
    "PositionIds = UInt32[np.typing.ArrayLike, \"seq_len\"]\n",
    "AttentionMask = Bool[np.typing.ArrayLike, \"seq_len seq_len\"]\n",
    "LossMask = Bool[np.typing.ArrayLike, \"seq_len\"]\n",
    "LMExample = dict[str, Inputs | Labels | PositionIds | AttentionMask | LossMask]\n",
    "\n",
    "\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, rows: list[TokenizedRow]):\n",
    "        super().__init__()\n",
    "        self.rows = rows\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> LMExample:\n",
    "        \"\"\"\n",
    "        Language modeling objective.\n",
    "        tokens: are input tokens, all except the last one\n",
    "        labels: target next tokens, all except the first one\n",
    "        attention_mask: since we're doing causal language modeling\n",
    "        loss_mask: whether loss should be calculated on specific `target` tokens\n",
    "        position_ids: positions of input tokens\n",
    "        \"\"\"\n",
    "        record = self.rows[idx]\n",
    "        text = record[\"input_ids\"]\n",
    "        # your code goes here\n",
    "\n",
    "        return {\n",
    "            \"tokens\": tokens.contiguous(),\n",
    "            \"labels\": labels.contiguous(),\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"loss_mask\": loss_mask,\n",
    "            \"position_ids\": position_ids,\n",
    "        }\n",
    "\n",
    "\n",
    "dataset = LMDataset(result)\n",
    "\n",
    "assert isinstance(dataset[0][\"tokens\"], torch.Tensor)\n",
    "assert dataset[0][\"tokens\"].shape[0] == 1024\n",
    "assert dataset[0][\"labels\"].shape[0] == 1024\n",
    "assert dataset[0][\"attention_mask\"].shape[1] == 1024\n",
    "assert dataset[0][\"loss_mask\"].shape[0] == 1024\n",
    "assert dataset[0][\"position_ids\"].shape[0] == 1024\n",
    "\n",
    "assert dataset[0][\"tokens\"][0] == 89963\n",
    "assert dataset[0][\"tokens\"][-1] == 360\n",
    "assert dataset[0][\"labels\"][0] == 52\n",
    "assert dataset[0][\"labels\"][-1] == 3045\n",
    "assert dataset[0][\"position_ids\"][-1] == 364"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ad30d-a60c-406d-aeb7-4817c59dbf57",
   "metadata": {},
   "source": [
    "Finally, a dataloader. We'll need to write a data collator that stacks all the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e2f809-470b-4470-ba3e-93c81d57ce54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "InputsBatch = UInt32[np.typing.ArrayLike, \"bs seq_len\"]\n",
    "LabelsBatch = UInt32[np.typing.ArrayLike, \"bs seq_len\"]\n",
    "PositionIdsBatch = UInt32[np.typing.ArrayLike, \"bs seq_len\"]\n",
    "AttentionMaskBatch = Bool[np.typing.ArrayLike, \"bs seq_len seq_len\"]\n",
    "LossMaskBatch = Bool[np.typing.ArrayLike, \"bs seq_len\"]\n",
    "LMExampleBatch = dict[str, InputsBatch | LabelsBatch | PositionIdsBatch | AttentionMaskBatch | LossMaskBatch]\n",
    "\n",
    "\n",
    "def collate_lm(batch: list[LMExample]) -> LMExampleBatch:\n",
    "    \"\"\"\n",
    "    Stacks all the examples in the batch\n",
    "    \"\"\"\n",
    "    # your code goes here\n",
    "\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    collate_fn=collate_lm,\n",
    ")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "assert batch[\"tokens\"].shape == (8, 1024)\n",
    "assert batch[\"labels\"].shape == (8, 1024)\n",
    "assert batch[\"position_ids\"].shape == (8, 1024)\n",
    "assert batch[\"attention_mask\"].ndim == 3 and batch[\"attention_mask\"].shape[0] == 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4ca092-6d27-4efb-8001-a626e5f78305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
